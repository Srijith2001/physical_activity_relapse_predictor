{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import required library and also load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "\n",
    "df = pd.read_csv(\"dailySteps_merged.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seperate Steps count from other datas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.pivot(index ='Id', columns ='ActivityDay', values =['StepTotal']) \n",
    "df.to_csv('first_table.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rename columns headers to Day%%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('first_table.csv',index_col=0)\n",
    "i = 1\n",
    "for col in df.columns:\n",
    "    df.rename(columns={col:'Day'+str(i)},inplace=True)\n",
    "    i= i +1\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Delete ActivityDay and ID row and also drop rows with more than 10 Null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(index=['ActivityDay','Id'])\n",
    "df = df.loc[df.isnull().sum(axis=1) <10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for no. of null values in a row\n",
    "df.isnull().sum(axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interpolate the remaining NaN values linearly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 1\n",
    "for col in df.columns:\n",
    "    df['Day'+ str(j)] = pd.to_numeric(df['Day'+str(j)], errors='coerce')\n",
    "    j = j + 1\n",
    "df = df.interpolate(method ='linear', limit_direction ='forward', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change all the step counts to integer value\n",
    "k = 1\n",
    "for col in df.columns:\n",
    "    df['Day'+str(k)] = df['Day'+str(k)].astype(np.int64)\n",
    "    k = k + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the dataset we are using is very small, we shall use permutation to create a synthetic dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from itertools import permutations\n",
    "\n",
    "c_count = -1\n",
    "for col in df.columns:\n",
    "    c_count = c_count + 1 \n",
    "    row_values = []\n",
    "    count = 0\n",
    "    for index in df.index:\n",
    "        if(count==2):\n",
    "            break\n",
    "        row_values.append(df.at[index,col])\n",
    "        count = count+1\n",
    "    perm = permutations([row_values[0], row_values[1]])\n",
    "    for i in perm:\n",
    "        for x in i:\n",
    "            t_count = 0\n",
    "            df2 = []\n",
    "            for columns in df.columns:\n",
    "                if(c_count == t_count):\n",
    "                    df2.append(x)\n",
    "                    t_count = t_count + 1\n",
    "                else:\n",
    "                    df2.append(df[columns].iloc[0])\n",
    "                    t_count = t_count + 1\n",
    "            df_length = len(df)\n",
    "            df.loc[df_length] = df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('initial_table.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('initial_table.csv',index_col=0)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a table with columns: 'Initial Steps','Average Steps', 'Last Week Steps', 'Day Number','DiPS'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame({'Initial Steps':[],'Average Steps':[], 'Last Week Steps':[], 'Day Number':[],'DiPS':[]})\n",
    "t_int = 0\n",
    "t_lws = 0\n",
    "t_avg = 0\n",
    "t_dno = 0\n",
    "t_dips = -1\n",
    "for index in df.index:\n",
    "    for col in df.columns:\n",
    "        if(col=='Day1'):\n",
    "            t_int = df.at[index,col]\n",
    "            continue\n",
    "        elif(col=='Day31'):\n",
    "            continue\n",
    "        else:\n",
    "            t_lws = df.at[index,col]\n",
    "            t_avg = abs((t_int + t_lws)/2)\n",
    "            t_dno = int(col[3:])\n",
    "            if(df.at[index,'Day'+str((int(col[3:])+1))]>t_int):\n",
    "                t_dips = 100\n",
    "            else:\n",
    "                t_dips = 0\n",
    "            final = final.append({'Initial Steps':t_int,'Average Steps':t_avg, 'Last Week Steps':t_lws,'Day Number':t_dno,'DiPS': t_dips}, ignore_index=True)\n",
    "\n",
    "final.drop_duplicates()\n",
    "final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('dataset.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification\n",
    "As our dataset is now ready we shall use Ensemble Classifier algorithms to train the model\n",
    "\n",
    "Since there are various different models, we shall try all of them one by one.\n",
    "\n",
    "## Bagging Algorithms\n",
    "\n",
    "### 1. Bagged Decision Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "#Load the dataset and assign X and Y\n",
    "dataset = pd.read_csv('dataset.csv')\n",
    "Y = dataset.DiPS\n",
    "features = ['Initial Steps','Average Steps','Last Week Steps','Day Number']\n",
    "X = dataset[features]\n",
    "\n",
    "#Initialize model and train it.\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "cart = DecisionTreeClassifier()\n",
    "b_model = BaggingClassifier(base_estimator=cart, n_estimators=100, random_state=0)\n",
    "\n",
    "# Find the mean prediction rate\n",
    "b_results = model_selection.cross_val_score(b_model, X, Y, cv=kfold)\n",
    "print(b_results.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "#Load the dataset and assign X and Y\n",
    "dataset = pd.read_csv('dataset.csv')\n",
    "Y = dataset.DiPS\n",
    "features = ['Initial Steps','Average Steps','Last Week Steps','Day Number']\n",
    "X = dataset[features]\n",
    "\n",
    "#Initialize model and train it.\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "r_model = RandomForestClassifier(n_estimators=100, max_features=4)\n",
    "\n",
    "# Find the mean prediction rate\n",
    "r_results = model_selection.cross_val_score(r_model, X, Y, cv=kfold)\n",
    "print(r_results.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Extra Trees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "\n",
    "#Load the dataset and assign X and Y\n",
    "dataset = pd.read_csv('dataset.csv')\n",
    "Y = dataset.DiPS\n",
    "features = ['Initial Steps','Average Steps','Last Week Steps','Day Number']\n",
    "X = dataset[features]\n",
    "\n",
    "#Initialize model and train it.\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "e_model = ExtraTreesClassifier(n_estimators=100, max_features=4)\n",
    "\n",
    "# Find the mean prediction rate\n",
    "e_results = model_selection.cross_val_score(e_model, X, Y, cv=kfold)\n",
    "print(e_results.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Boosting Algorithms\n",
    "\n",
    "### 1. AdaBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "#Load the dataset and assign X and Y\n",
    "dataset = pd.read_csv('dataset.csv')\n",
    "Y = dataset.DiPS\n",
    "features = ['Initial Steps','Average Steps','Last Week Steps','Day Number']\n",
    "X = dataset[features]\n",
    "\n",
    "#Initialize model and train it.\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "ab_model = AdaBoostClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "# Find the mean prediction rate\n",
    "ab_results = model_selection.cross_val_score(ab_model, X, Y, cv=kfold)\n",
    "print(ab_results.mean())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Stochastic Gradient Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "#Load the dataset and assign X and Y\n",
    "dataset = pd.read_csv('dataset.csv')\n",
    "Y = dataset.DiPS\n",
    "features = ['Initial Steps','Average Steps','Last Week Steps','Day Number']\n",
    "X = dataset[features]\n",
    "\n",
    "#Initialize model and train it.\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "gb_model = GradientBoostingClassifier(n_estimators=100, random_state=0)\n",
    "\n",
    "# Find the mean prediction rate\n",
    "gb_results = model_selection.cross_val_score(gb_model, X, Y, cv=kfold)\n",
    "print(gb_results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Voting Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "#Load the dataset and assign X and Y\n",
    "dataset = pd.read_csv('dataset.csv')\n",
    "Y = dataset.DiPS\n",
    "features = ['Initial Steps','Average Steps','Last Week Steps','Day Number']\n",
    "X = dataset[features]\n",
    "\n",
    "kfold = model_selection.KFold(n_splits=10)\n",
    "\n",
    "# Create the sub models\n",
    "estimators = []\n",
    "model1 = LogisticRegression()\n",
    "estimators.append(('logistic', model1))\n",
    "model2 = DecisionTreeClassifier()\n",
    "estimators.append(('cart', model2))\n",
    "model3 = SVC()\n",
    "estimators.append(('svm', model3))\n",
    "\n",
    "# Create the ensemble model\n",
    "ensemble = VotingClassifier(estimators)\n",
    "\n",
    "# Find the mean prediction rate\n",
    "ve_results = model_selection.cross_val_score(ensemble, X, Y, cv=kfold)\n",
    "print(ve_results.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we have tried various different ensemble classifiers, let us summarize the prediction accuracy of each model.\n",
    "\n",
    "## Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Mean Prediction Accuracy of various models:\")\n",
    "print(\"\\nBagging Algorithms\")\n",
    "print(\"1. Bagged Decision Trees: \",b_results.mean())\n",
    "print(\"2. Random Forest: \",r_results.mean())\n",
    "print(\"3. Extra Trees: \",e_results.mean())\n",
    "print('\\nBoosting Algorithms')\n",
    "print(\"1. AdaBoost: \",ab_results.mean())\n",
    "print(\"2. Stochastic Gradient Boosting \",gb_results.mean())\n",
    "print('\\nVoting Ensemble:',ve_results.mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
